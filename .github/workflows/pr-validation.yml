name: PR Validation
# Runs on pull requests - tests, linting, build, and benchmarks
# Validates: unit tests (80% coverage), code quality, Docker build, accuracy (80%), citations (100%)

on:
  pull_request:
    types: [opened, synchronize, reopened]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  PYTHON_VERSION: '3.11'

concurrency:
  group: pr-${{ github.event.pull_request.number }}
  cancel-in-progress: true

jobs:
  # ========================================================================
  # JOB 1: SETUP - Prepare environment, restore caches
  # ========================================================================
  setup:
    name: Setup
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better caching

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Set up Docker BuildKit
        uses: docker/setup-buildx-action@v3

      - name: Display environment info
        run: |
          echo "Python version: $(python --version)"
          echo "Pip version: $(pip --version)"
          echo "Docker BuildKit: $(docker buildx version)"

  # ========================================================================
  # JOB 2: TESTS - Run unit tests with coverage gate (80% required)
  # ========================================================================
  tests:
    name: Tests & Coverage
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run tests with coverage
        run: |
          pytest tests/ \
            --cov=src/ \
            --cov-report=json:coverage.json \
            --cov-report=term \
            --cov-report=html:htmlcov \
            --junit-xml=test-results.xml \
            -v
        continue-on-error: false

      - name: Check coverage threshold
        run: |
          COVERAGE=$(python -c "import json; print(json.load(open('coverage.json'))['totals']['percent_covered'])")
          echo "Coverage: ${COVERAGE}%"
          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "‚ùå Coverage below 80% threshold (${COVERAGE}%)"
            exit 1
          fi
          echo "‚úÖ Coverage meets threshold (${COVERAGE}% >= 80%)"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results.xml
            coverage.json
            htmlcov/
          retention-days: 30

      - name: Publish test report
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Test Results
          path: 'test-results.xml'
          reporter: 'java-junit'

  # ========================================================================
  # JOB 3: LINT - Code quality checks (format, linting, types, security)
  # ========================================================================
  lint:
    name: Code Quality
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install black ruff mypy bandit pip-audit

      - name: Check formatting with black
        run: |
          echo "Checking code formatting with black..."
          black --check src/ tests/ || {
            echo "‚ùå Code formatting issues found. Run 'black src/ tests/' to fix."
            black --diff src/ tests/
            exit 1
          }
          echo "‚úÖ Code formatting OK"

      - name: Run ruff linter
        run: |
          echo "Running ruff linter..."
          ruff check src/ tests/ --output-format=json > ruff-output.json || {
            echo "‚ùå Ruff violations found"
            ruff check src/ tests/
            exit 1
          }
          echo "‚úÖ Ruff checks passed"

      - name: Run mypy type checker
        run: |
          echo "Running mypy type checker..."
          mypy src/ --json > mypy-output.json 2>&1 || {
            echo "‚ùå Type errors found"
            mypy src/
            exit 1
          }
          echo "‚úÖ Type checking passed"

      - name: Security scan with bandit
        run: |
          echo "Running bandit security scan..."
          bandit -r src/ -f json -o bandit-output.json || {
            echo "‚ùå Security issues found"
            bandit -r src/
            exit 1
          }
          echo "‚úÖ Security scan passed"

      - name: Audit dependencies with pip-audit
        run: |
          echo "Auditing dependencies with pip-audit..."
          pip-audit --desc > pip-audit-output.json 2>&1 || {
            echo "‚ö†Ô∏è  Dependency vulnerabilities detected"
            pip-audit --desc
            exit 1
          }
          echo "‚úÖ Dependency audit passed"

      - name: Upload quality reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-reports
          path: |
            ruff-output.json
            mypy-output.json
            bandit-output.json
            pip-audit-output.json
          retention-days: 30

  # ========================================================================
  # JOB 4: BUILD - Docker image build and push to ghcr.io
  # Runs only after tests and lint pass
  # ========================================================================
  build:
    name: Build Docker Image
    needs: [tests, lint]
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker BuildKit
        uses: docker/setup-buildx-action@v3

      - name: Generate image tag
        id: meta
        run: |
          BRANCH_NAME=$(echo "${{ github.event.pull_request.head.ref }}" | sed 's/[^a-zA-Z0-9._-]/-/g')
          COMMIT_SHORT=$(git rev-parse --short HEAD)
          TAG="pr-${BRANCH_NAME}-${COMMIT_SHORT}"
          echo "tag=${TAG}" >> $GITHUB_OUTPUT
          echo "full_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${TAG}" >> $GITHUB_OUTPUT
          echo "Image tag: ${TAG}"

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.full_image }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            PYTHON_VERSION=${{ env.PYTHON_VERSION }}

      - name: Get image digest
        id: digest
        run: |
          DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' ${{ steps.meta.outputs.full_image }} 2>/dev/null || echo "unknown")
          echo "digest=${DIGEST}" >> $GITHUB_OUTPUT
          echo "Image digest: ${DIGEST}"

      - name: Run Trivy vulnerability scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.meta.outputs.full_image }}
          format: 'json'
          output: 'trivy-results.json'
          severity: 'CRITICAL,HIGH'

      - name: Check Trivy scan results
        run: |
          CRITICAL=$(jq '[.Results[]?.Misconfigurations[]? | select(.Severity=="CRITICAL")] | length' trivy-results.json 2>/dev/null || echo 0)
          HIGH=$(jq '[.Results[]?.Misconfigurations[]? | select(.Severity=="HIGH")] | length' trivy-results.json 2>/dev/null || echo 0)
          
          if [ "$CRITICAL" -gt 0 ] || [ "$HIGH" -gt 0 ]; then
            echo "‚ùå Trivy found vulnerabilities: CRITICAL=$CRITICAL, HIGH=$HIGH"
            jq . trivy-results.json
            exit 1
          fi
          echo "‚úÖ Trivy scan passed (no critical/high vulnerabilities)"

      - name: Save build metadata
        run: |
          cat > build-metadata.json << EOF
          {
            "image": "${{ steps.meta.outputs.full_image }}",
            "tag": "${{ steps.meta.outputs.tag }}",
            "digest": "${{ steps.digest.outputs.digest }}",
            "timestamp": "$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          }
          EOF

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            build-metadata.json
            trivy-results.json
          retention-days: 30

  # ========================================================================
  # JOB 5: BENCHMARK - Run benchmark suite with accuracy and citation gates
  # Runs only after build passes
  # Skips for docs-only changes (if supported)
  # ========================================================================
  benchmark:
    name: Benchmark & Accuracy Gates
    needs: build
    runs-on: ubuntu-latest
    timeout-minutes: 35
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Generate image tag
        id: meta
        run: |
          BRANCH_NAME=$(echo "${{ github.event.pull_request.head.ref }}" | sed 's/[^a-zA-Z0-9._-]/-/g')
          COMMIT_SHORT=$(git rev-parse --short HEAD)
          TAG="pr-${BRANCH_NAME}-${COMMIT_SHORT}"
          echo "full_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${TAG}" >> $GITHUB_OUTPUT

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull Docker image
        run: docker pull ${{ steps.meta.outputs.full_image }}

      - name: Start API container
        run: |
          docker run -d \
            --name hive-api \
            -p 8000:8000 \
            -e ENV=test \
            ${{ steps.meta.outputs.full_image }}
          echo "API container started, waiting for health check..."

      - name: Wait for API health check
        run: |
          for i in {1..30}; do
            if curl -sf http://localhost:8000/health > /dev/null 2>&1; then
              echo "‚úÖ API is healthy"
              exit 0
            fi
            echo "Attempt $i/30: waiting for API..."
            sleep 2
          done
          echo "‚ùå API startup timeout"
          docker logs hive-api
          exit 1

      - name: Run benchmark suite
        run: |
          python -m tests.benchmark.benchmark \
            --api-url http://localhost:8000 \
            --suite full \
            --output-json benchmark-results.json \
            --output-junit benchmark-results.xml \
            -v
        continue-on-error: false

      - name: Check accuracy gate (‚â•80%)
        run: |
          ACCURACY=$(python -c "import json; print(json.load(open('benchmark-results.json'))['accuracy'])")
          echo "Accuracy: ${ACCURACY}%"
          if (( $(echo "$ACCURACY < 80" | bc -l) )); then
            echo "‚ùå Accuracy below 80% threshold (${ACCURACY}%)"
            exit 1
          fi
          echo "‚úÖ Accuracy meets threshold (${ACCURACY}% >= 80%)"

      - name: Check citations gate (100%)
        run: |
          CITATIONS=$(python -c "import json; print(json.load(open('benchmark-results.json'))['citations_accuracy'])")
          echo "Citations accuracy: ${CITATIONS}%"
          if (( $(echo "$CITATIONS < 100" | bc -l) )); then
            echo "‚ùå Citations below 100% (${CITATIONS}%)"
            exit 1
          fi
          echo "‚úÖ Citations at 100%"

      - name: Check regression (vs baseline)
        run: |
          # TODO: Fetch baseline from main branch artifact
          # For now, just log current accuracy
          ACCURACY=$(python -c "import json; print(json.load(open('benchmark-results.json'))['accuracy'])")
          echo "Current accuracy: ${ACCURACY}%"
          echo "Baseline comparison would go here (Phase 5 enhancement)"

      - name: Parse benchmark results for PR comment
        if: always()
        run: |
          python << 'PYTHON_SCRIPT'
          import json
          
          with open('benchmark-results.json') as f:
              results = json.load(f)
          
          comment = f"""## üìà Benchmark Results

          | Metric | Value | Status |
          |--------|-------|--------|
          | Accuracy | {results['accuracy']:.1f}% | {'‚úÖ' if results['accuracy'] >= 80 else '‚ùå'} |
          | Citations | {results['citations_accuracy']:.1f}% | {'‚úÖ' if results['citations_accuracy'] == 100 else '‚ùå'} |
          | P50 Latency | {results['latency']['p50']:.0f}ms | ‚úÖ |
          | P95 Latency | {results['latency']['p95']:.0f}ms | {'‚úÖ' if results['latency']['p95'] < 1000 else '‚ö†Ô∏è'} |
          | P99 Latency | {results['latency']['p99']:.0f}ms | {'‚úÖ' if results['latency']['p99'] < 2000 else '‚ö†Ô∏è'} |
          """
          
          with open('benchmark-comment.md', 'w') as f:
              f.write(comment)
          PYTHON_SCRIPT

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            benchmark-results.xml
            benchmark-comment.md
          retention-days: 30

      - name: Stop API container
        if: always()
        run: |
          docker stop hive-api 2>/dev/null || true
          docker rm hive-api 2>/dev/null || true

  # ========================================================================
  # FINAL: Post PR comment with summary
  # ========================================================================
  pr-comment:
    name: Post PR Comment
    if: always()
    needs: [tests, lint, build, benchmark]
    runs-on: ubuntu-latest
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Build PR comment
        id: comment
        run: |
          COMMENT="## ‚úÖ CI/CD Validation Summary\n\n"
          
          # Tests section
          if [ -f "test-results/test-results.xml" ]; then
            COMMENT+="### üß™ Tests\n"
            # Extract test count from XML (basic parsing)
            TESTS=$(grep -c "testcase" test-results/test-results.xml || echo "0")
            COMMENT+="- Tests: $TESTS passed ‚úÖ\n"
            if [ -f "test-results/coverage.json" ]; then
              COVERAGE=$(python -c "import json; print(json.load(open('test-results/coverage.json'))['totals']['percent_covered'])")
              COMMENT+="- Coverage: ${COVERAGE}% ‚úÖ\n"
            fi
            COMMENT+="\n"
          fi
          
          # Lint section
          if [ -f "quality-reports/ruff-output.json" ]; then
            COMMENT+="### üîç Code Quality\n"
            COMMENT+="- Format: ‚úÖ\n"
            COMMENT+="- Linting: ‚úÖ\n"
            COMMENT+="- Type checking: ‚úÖ\n"
            COMMENT+="- Security: ‚úÖ\n"
            COMMENT+="- Dependencies: ‚úÖ\n"
            COMMENT+="\n"
          fi
          
          # Build section
          if [ -f "build-artifacts/build-metadata.json" ]; then
            COMMENT+="### üê≥ Docker Build\n"
            IMAGE=$(python -c "import json; print(json.load(open('build-artifacts/build-metadata.json'))['image'])")
            COMMENT+="- Image: \`${IMAGE}\` ‚úÖ\n"
            COMMENT+="- Trivy scan: ‚úÖ\n"
            COMMENT+="\n"
          fi
          
          # Benchmark section
          if [ -f "benchmark-results/benchmark-comment.md" ]; then
            COMMENT+="$(cat benchmark-results/benchmark-comment.md)\n"
          fi
          
          COMMENT+="---\n"
          COMMENT+="**All checks passed!** Ready to merge. üöÄ"
          
          echo "comment_body<<EOF" >> $GITHUB_OUTPUT
          echo -e "$COMMENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Post comment on PR
        if: github.event.pull_request
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${{ steps.comment.outputs.comment_body }}`
            })
