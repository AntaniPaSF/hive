# RAG LLM Service - Environment Configuration
# =============================================
# Copy this file to .env and configure for your environment:
#   cp .env.example .env
#
# For Docker deployments, these values can also be set in docker-compose.yml
#
# IMPORTANT: Never commit .env to version control!

# ==================================
# Ollama Configuration
# ==================================
# URL for Ollama API (local or containerized)
# - Development: http://localhost:11434
# - Docker Compose: http://ollama:11434 (service name)
# - Remote: http://ollama-server.example.com:11434
OLLAMA_HOST=http://localhost:11434

# Ollama model to use for generation
# Options: mistral:7b (recommended), llama2:7b, phi:latest, codellama:7b
# Must be downloaded first: docker exec ollama-service ollama pull mistral:7b
OLLAMA_MODEL=mistral:7b

# ==================================
# Vector Store Configuration
# ==================================
# ChromaDB vector store URL (managed by HR Data Pipeline)
VECTOR_STORE_URL=http://localhost:8001

# Collection name in ChromaDB containing document embeddings
VECTOR_STORE_COLLECTION=hr_documents

# Number of chunks to retrieve per query (top-k)
MAX_RETRIEVED_CHUNKS=5

# ==================================
# Embedding Configuration
# ==================================
# HR Data Pipeline embedding API endpoint
# Provides 384-dim all-MiniLM-L6-v2 embeddings
EMBEDDING_API_URL=http://localhost:8002/embed

# Embedding model name (for validation/logging)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Expected embedding dimension (384 for all-MiniLM-L6-v2)
EMBEDDING_DIMENSION=384

# ==================================
# RAG Service Configuration
# ==================================
# Minimum confidence threshold for returning answers (0.0-1.0)
# Queries with avg similarity < threshold return "I don't know"
# - 0.3: Very permissive (may return low-quality answers)
# - 0.5: Balanced (recommended default)
# - 0.7: Strict (only high-confidence answers)
MIN_CONFIDENCE_THRESHOLD=0.5

# Service port for FastAPI server
# Must match docker-compose.yml port mapping
RAG_SERVICE_PORT=8000

# ==================================
# Logging Configuration
# ==================================
# Log level: DEBUG (verbose), INFO (default), WARNING, ERROR, CRITICAL
# Use DEBUG for troubleshooting, INFO for production
LOG_LEVEL=INFO

# Log output format: 
# - json: Structured JSON logs (recommended for production)
# - text: Human-readable text logs (useful for development)
LOG_FORMAT=json

# Log file path (optional)
# Leave empty for stdout only (recommended for Docker)
# Example: /app/logs/rag-service.log
LOG_FILE=

# ==================================
# API Configuration
# ==================================
# Enable CORS (for FastAPI wrapper integration)
ENABLE_CORS=true

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# ==================================
# Timeout Configuration
# ==================================
# Ollama generation timeout (seconds)
OLLAMA_TIMEOUT=30

# Vector store query timeout (seconds)
VECTOR_STORE_TIMEOUT=10

# Embedding API timeout (seconds)
EMBEDDING_API_TIMEOUT=5

# ==================================
# Retry Configuration
# ==================================
# Max retries for failed API calls
MAX_RETRIES=3

# Retry backoff multiplier (exponential backoff)
RETRY_BACKOFF=2
